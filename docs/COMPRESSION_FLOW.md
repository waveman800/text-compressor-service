"""
压缩服务与大模型上下文窗口关联 - 可视化流程说明
==================================================

## 1. 整体架构图

```
┌─────────────────────────────────────────────────────────────┐
│                     用户请求                                 │
│  (prompt + context/chat_history + max_new_tokens)           │
└────────────────────┬────────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────────┐
│              LLMInferenceEngine                             │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  1. 计算当前token数                                   │  │
│  │     - prompt_tokens: 提示词token数                     │  │
│  │     - text_tokens: 文本token数                        │  │
│  │     - max_new_tokens: 预留生成token数                  │  │
│  └────────────────────┬───────────────────────────────────┘  │
│                       │                                       │
│                       ▼                                       │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  2. 判断是否需要压缩                                  │  │
│  │     total_tokens = prompt_tokens + text_tokens +     │  │
│  │                    max_new_tokens                     │  │
│  │                                                      │  │
│  │     if total_tokens > session_len:                   │  │
│  │         需要压缩                                     │  │
│  │     else:                                            │  │
│  │         无需压缩，直接推理                            │  │
│  └────────────────────┬───────────────────────────────────┘  │
│                       │                                       │
│                       ▼ (如果需要压缩)                        │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  3. 计算可用token数                                  │  │
│  │     available_tokens = session_len -                 │  │
│  │                        prompt_tokens -                │  │
│  │                        max_new_tokens                 │  │
│  └────────────────────┬───────────────────────────────────┘  │
│                       │                                       │
│                       ▼                                       │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  4. 执行动态压缩                                      │  │
│  │     - 检测文本类型                                    │  │
│  │     - 选择压缩策略                                    │  │
│  │     - 压缩到available_tokens以内                      │  │
│  └────────────────────┬───────────────────────────────────┘  │
│                       │                                       │
│                       ▼                                       │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  5. 使用压缩后的文本进行推理                         │  │
│  └────────────────────┬───────────────────────────────────┘  │
└───────────────────────┼───────────────────────────────────────┘
                        │
                        ▼
┌─────────────────────────────────────────────────────────────┐
│                   大模型推理                                 │
│              (实际调用模型API)                               │
└────────────────────┬────────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────────┐
│                   返回结果                                   │
│  (生成的文本 + 压缩指标)                                     │
└─────────────────────────────────────────────────────────────┘
```

## 2. 动态压缩触发条件详解

### 触发公式
```python
total_tokens = prompt_tokens + text_tokens + max_new_tokens

if total_tokens > session_len:
    # 触发压缩
    available_tokens = session_len - prompt_tokens - max_new_tokens
    compressed_text = compress_to_fit(text, available_tokens)
else:
    # 无需压缩
    compressed_text = text
```

### 参数说明

| 参数 | 说明 | 示例值 |
|------|------|--------|
| session_len | 模型上下文窗口最大token数 | 4096 |
| prompt_tokens | 当前提示词的token数 | 256 |
| text_tokens | 待压缩文本的token数 | 4000 |
| max_new_tokens | 预留给生成新内容的token数 | 256 |
| total_tokens | 总token数 | 4512 |
| available_tokens | 压缩后可用的最大token数 | 3584 |

### 触发示例

#### 示例1：需要压缩
```
session_len = 4096
prompt_tokens = 256
text_tokens = 4000
max_new_tokens = 256

total_tokens = 256 + 4000 + 256 = 4512

因为 4512 > 4096，所以需要压缩

available_tokens = 4096 - 256 - 256 = 3584

需要将4000个token的文本压缩到3584个token以内
```

#### 示例2：无需压缩
```
session_len = 4096
prompt_tokens = 256
text_tokens = 3000
max_new_tokens = 256

total_tokens = 256 + 3000 + 256 = 3512

因为 3512 <= 4096，所以无需压缩

直接使用原始文本进行推理
```

## 3. 压缩策略选择流程

```
输入文本
    │
    ▼
┌─────────────────────────────┐
│  检测文本类型                │
└──────────┬──────────────────┘
           │
           ▼
    ┌──────────────┐
    │  对话类型？   │
    └──┬────────┬──┘
       │是      │否
       ▼        ▼
┌──────────┐  ┌──────────────┐
│对话压缩  │  │  列表类型？   │
└──────────┘  └──┬────────┬──┘
                  │是      │否
                  ▼        ▼
             ┌──────────┐  ┌──────────────┐
             │列表压缩  │  │  代码类型？   │
             └──────────┘  └──┬────────┬──┘
                               │是      │否
                               ▼        ▼
                          ┌──────────┐  ┌──────────┐
                          │代码压缩  │  │叙述压缩  │
                          └──────────┘  └──────────┘
```

## 4. 叙述类型压缩流程

```
输入文本
    │
    ▼
┌─────────────────────────────┐
│  按段落分割文本              │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  计算每个段落的重要性        │
│  - 关键词匹配 (70%)         │
│  - TF-IDF分析 (30%)         │
│  - 位置权重 (25%)           │
│  - 长度权重 (15%)           │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  按重要性排序段落            │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  选择高重要性段落            │
│  直到达到available_tokens   │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  恢复原始顺序                │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  重建压缩后的文本            │
└─────────────────────────────┘
```

## 5. 聊天历史压缩流程

```
聊天历史
    │
    ▼
┌─────────────────────────────┐
│  提取最新消息作为提示词       │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  分离历史消息和最新消息       │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  倒序处理历史消息            │
│  (从最新的开始)              │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  保留最新的消息              │
│  直到达到available_tokens   │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  压缩最早的消息              │
│  (如果还有剩余空间)          │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  合并压缩后的历史和最新消息   │
└─────────────────────────────┘
```

## 6. 实际使用场景

### 场景1：长文档问答
```
用户请求：总结这个长文档的关键点
文档长度：5000 tokens
提示词：50 tokens
max_new_tokens：256 tokens

session_len = 4096
total_tokens = 50 + 5000 + 256 = 5306 > 4096

触发压缩！
available_tokens = 4096 - 50 - 256 = 3790

将5000 tokens的文档压缩到3790 tokens以内
```

### 场景2：长对话历史
```
对话历史：30轮对话，共8000 tokens
最新消息：100 tokens
max_new_tokens：512 tokens

session_len = 8192
total_tokens = 100 + 8000 + 512 = 8612 > 8192

触发压缩！
available_tokens = 8192 - 100 - 512 = 7580

压缩策略：
1. 保留最新的10轮对话（约3000 tokens）
2. 压缩最早的20轮对话到4580 tokens以内
```

### 场景3：代码分析
```
代码文件：3000 tokens
问题描述：200 tokens
max_new_tokens：512 tokens

session_len = 4096
total_tokens = 200 + 3000 + 512 = 3712 <= 4096

无需压缩！
直接使用原始代码进行分析
```

## 7. 性能优化建议

### 优化1：快速压缩模式
```python
engine = LLMInferenceEngine(
    model_name="gpt-3.5-turbo",
    context_length=4096,
    max_output_tokens=256,
    use_fast_compression=True  # 启用快速压缩
)
```

### 优化2：异步处理
```python
# 使用异步方法减少阻塞
output = await engine.generate_async(prompt, context)
```

### 优化3：批量处理
```python
# 批量压缩多个文本
results = await engine.compressor.batch_compress(
    texts=[text1, text2, text3],
    current_prompt=prompt,
    max_new_tokens=256
)
```

### 优化4：预压缩常用文本
```python
# 预压缩常用文档，减少实时计算
pre_compressed_docs = {
    "doc1": compressor.dynamic_compress(doc1, "", 256)[0],
    "doc2": compressor.dynamic_compress(doc2, "", 256)[0],
}
```

## 8. 监控和调优

### 监控指标
```python
metrics = engine.get_compression_metrics(original_text, compressed_text)

print(f"压缩比: {metrics['compression_ratio']:.2%}")
print(f"关键词保留率: {metrics['keyword_retention']:.2%}")
print(f"信息保留率: {metrics['information_preservation']:.2%}")
print(f"处理时间: {metrics['processing_time']:.4f}秒")
```

### 调优建议

1. **调整session_len**
   - 根据模型实际能力设置
   - 留出10-20%安全余量

2. **调整max_new_tokens**
   - 根据预期输出长度设置
   - 避免设置过大浪费空间

3. **优化关键词列表**
   - 根据应用场景定制
   - 提高压缩质量

4. **选择合适的压缩模式**
   - 实时场景：快速压缩模式
   - 离线场景：标准压缩模式

## 9. 常见问题排查

### 问题1：压缩后信息丢失严重
**原因**：关键词列表不匹配应用场景
**解决**：定制关键词列表，添加领域特定关键词

### 问题2：压缩速度太慢
**原因**：文本过长或压缩模式不适合
**解决**：启用快速压缩模式或使用异步处理

### 问题3：压缩后仍然超出上下文窗口
**原因**：max_new_tokens设置过大
**解决**：减少max_new_tokens或增加session_len

### 问题4：压缩质量不稳定
**原因**：文本类型识别不准确
**解决**：手动指定压缩策略或优化文本类型检测

## 10. 最佳实践

1. **合理设置参数**
   - session_len：根据模型能力设置
   - max_new_tokens：根据输出需求设置
   - enable_compression：根据场景决定是否启用

2. **监控压缩质量**
   - 定期检查压缩指标
   - 根据指标调整策略

3. **选择合适的压缩模式**
   - 实时场景：快速压缩
   - 离线场景：标准压缩

4. **优化关键词列表**
   - 根据应用场景定制
   - 定期更新和优化

5. **使用异步处理**
   - 减少阻塞
   - 提高并发性能
